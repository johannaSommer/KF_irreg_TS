{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greatest-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rental-charger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NegLL tensor([-0.2324])\n",
      "Likelihood tensor([1.2616])\n"
     ]
    }
   ],
   "source": [
    "dist = torch.distributions.Normal(0, torch.sqrt(torch.Tensor([0.1])))\n",
    "print(\"NegLL\", -dist.log_prob(0))\n",
    "print(\"Likelihood\", torch.exp(dist.log_prob(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "jewish-gallery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NegLL tensor(-0.4647)\n",
      "Likelihood tensor(1.5915)\n",
      "Normalized NegLL tensor(-0.2324)\n",
      "Normalized Likelihood tensor(1.2616)\n"
     ]
    }
   ],
   "source": [
    "dist = torch.distributions.MultivariateNormal(torch.Tensor([0, 0]), torch.Tensor([[0.1, 0], [0, 0.1]]))\n",
    "print(\"NegLL\", -dist.log_prob(torch.Tensor([0, 0])))\n",
    "print(\"Likelihood\", torch.exp(dist.log_prob(torch.Tensor([0, 0]))))\n",
    "print(\"Normalized NegLL\", -dist.log_prob(torch.Tensor([0, 0])) / 2)\n",
    "print(\"Normalized Likelihood\", torch.exp(dist.log_prob(torch.Tensor([0, 0])) / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-chest",
   "metadata": {},
   "source": [
    "## 1D Normal Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-silver",
   "metadata": {},
   "source": [
    "For the log-likelihood for a single sample in a Normal distribution we have: </br>\n",
    "$ ll = -0.5 log(2\\pi) - 0.5 log(\\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2}$ </br>\n",
    "When we treat a multidimensional sample as a Normal distribution, we calculate the individual log-likelihood for d samples and take the mean, which gives us </br>\n",
    "$ LL = \\frac{1}{d} \\sum^d_{i=0} -0.5 log(2\\pi) - 0.5 log(\\sigma_i^2) - \\frac{(x_i - \\mu_i)^2}{2\\sigma_i^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-thanks",
   "metadata": {},
   "source": [
    "## Multivariate Normal Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-cradle",
   "metadata": {},
   "source": [
    "For the log-likelihood of a single observation with d observed dimensions we get: </br>\n",
    "$LL = -0.5*d*log(2\\pi) - 0.5log(|\\sum|) - 0.5MD(x, \\mu, \\sum)^2$ </br>\n",
    "where MD denotes the Mahalanobis distance.\n",
    "Now, if we assume diagonal covariance, we see that the determinant of the covariance matrix is simply a summation over the logged diagonal elements, which gives </br>\n",
    "$LL = -0.5\\sum^d*log(2\\pi) - 0.5\\sum^dlog(\\sum_{i,i}) - 0.5 MD(x, \\mu, \\sum)^2$ </br>\n",
    "Again, if we assume diagonal covariance, the Mahalanobis distance reduces to a standardized euclidian distance: </br>\n",
    "$LL = -0.5\\sum^d*log(2\\pi) - 0.5\\sum^dlog(\\sum_{i,i}) - (\\sum_{i=0}^d \\sqrt(\\frac{(x_i - \\mu_i)}{2\\sigma_i^2}))^2$ </br>\n",
    "$LL = -0.5\\sum^d*log(2\\pi) - 0.5\\sum^dlog(\\sum_{i,i}) - \\sum_{i=0}^d \\frac{(x_i - \\mu_i)}{2\\sigma_i^2}$ </br>\n",
    "We can pull out the sum, get </br>\n",
    "$LL = \\sum_{i=0}^d -0.5log(2\\pi) - 0.5log(\\sum_{i,i}) - \\frac{(x_i - \\mu_i)}{2\\sum_{i,i}^2}$ </br>\n",
    "and see that we indeed have to normalize by $\\frac{1}{d}$ to have comparable metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
